
# Geometric Distances as Objective Functions

## 1. The Minkowski Family (Flat Space Geometry)
These measure distance in standard, flat Euclidean space. The general formula is $D(x, y) = (\sum |x_i - y_i|^p)^{1/p}$.

### A. Euclidean Distance ($L_2$ Norm)
*   **The Geometry:** The "As the crow flies" straight-line distance. The unit ball is a **Circle/Sphere**.
*   **The Loss Function:** Mean Squared Error (MSE).
    $L = \frac{1}{N} \sum (y_{pred} - y_{true})^2$
*   **The Inductive Bias:**
    *   **Physics:** It assumes errors are distributed normally (Gaussian Noise).
    *   **Effect:** It penalizes outliers heavily (because of the square). The model tries to minimize the *average* error, leading to smooth, blurry outputs in image generation.
*   **Application:** Regression, Autoencoder reconstruction, Style Transfer.

### B. Manhattan Distance ($L_1$ Norm)
*   **The Geometry:** The "Taxicab" distance. You can only move along grid lines (X and Y axes). The unit ball is a **Diamond**.
*   **The Loss Function:** Mean Absolute Error (MAE).
    $$L = \frac{1}{N} \sum |y_{pred} - y_{true}|$$
*   **The Inductive Bias:**
    *   **Physics:** It assumes Laplacian noise (spiky distribution).
    *   **Effect:** It is robust to outliers. More importantly, it induces **Sparsity**. Because the unit ball is a diamond with sharp corners on the axes, optimization tends to push weights to exactly Zero.
*   **Application:** Image de-blurring (produces sharper edges than $L_2$), Lasso Regression, Compressed Sensing.

---

## 2. Directional Geometry (Spherical Space)
In very high dimensions, "distance" becomes meaningless (Curse of Dimensionality), but "angle" remains robust.

### Cosine Distance
*   **The Geometry:** Measures the angle between two vectors, ignoring their length (magnitude). It projects everything onto a **Hypersphere**.
    $$D_{cos}(x, y) = 1 - \frac{x \cdot y}{\|x\| \|y\|}$$
*   **The Inductive Bias:**
    *   **Semantics over Intensity:** In NLP, the vector "King" and "King" (shouted loudly / large magnitude) should be the same. The direction holds the meaning; the magnitude holds the frequency/intensity.
*   **Application:**
    *   **Contrastive Learning (CLIP, SimCLR):** Pushing "similar" images to align on the sphere and "dissimilar" images to be orthogonal.
    *   **Face Recognition (ArcFace):** Mapping faces to a sphere where all faces of the same person are clustered in a narrow angle.

---

## 3. Set Geometry (Point Cloud Space)
How do you measure distance between two *unordered sets* of points (e.g., two 3D scans of a chair)?

### Chamfer Distance
*   **The Geometry:** For every point in Set A, find the nearest neighbor in Set B, and sum those distances. Then do the reverse and add them.
    $$ D_{CD}(S_1, S_2) = \sum_{x \in S_1} \min_{y \in S_2} \|x-y\|^2 + \sum_{y \in S_2} \min_{x \in S_1} \|x-y\|^2 $$
*   **The Inductive Bias:**
    *   It forces the *shapes* to overlap without requiring a specific point-to-point ordering.
*   **Application:** 3D Deep Learning (PointNet), LiDAR processing, Robotics.

---

## 4. Non-Euclidean Geometry (Curved Space)
Real-world data often has a hierarchical structure (like a family tree or file system) that creates massive distortion when smashed into flat (Euclidean) space.

### Hyperbolic Distance (Poincar√© Ball)
*   **The Geometry:** A space with constant negative curvature. As you move toward the edge of the disk, distances grow exponentially relative to Euclidean space.
    *   *Analogy:* Think of an M.C. Escher "Circle Limit" print. The angels near the edge look small to us, but in their world, they are the same size as the center ones. The space "expands" as you go out.
*   **The Inductive Bias:**
    *   **Exponential Capacity:** The volume of a hyperbolic disk increases exponentially with radius, just like the number of nodes in a tree increases exponentially with depth.
    *   **Hierarchical Embedding:** It can embed a massive taxonomy (WordNet) into just 2 dimensions with low distortion, whereas Euclidean space might need 200 dimensions.
*   **Application:** Hyperbolic Neural Networks, Graph embeddings for social networks, biological phylogenetics.

---

## Summary: Matching Metric to Manifold

| Geometry | Distance Measure | Training Objective | Best Used For |
| :--- | :--- | :--- | :--- |
| **Flat** | Euclidean ($L_2$) | Minimize Variance | Continuous values, regression |
| **Grid/Sparse** | Manhattan ($L_1$) | Minimize Deviations | Sharp edges, feature selection |
| **Spherical** | Cosine | Maximize Alignment | Semantic similarity, text/face embeddings |
| **Statistical** | Mahalanobis | Decorrelate Features | Anomaly detection, clustering |
| **Curved (Tree)** | Hyperbolic | Minimize Geodesic | Hierarchies, graphs, taxonomies |
| **Transport** | Wasserstein | Minimize Work | Distribution matching, GANs |

